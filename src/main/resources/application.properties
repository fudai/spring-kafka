spring.application.name=spring-kafka

# 公共配置
# Kafka服务器地址 多个地址用英文逗号分隔
spring.kafka.bootstrap-servers=host1:port1,host2:port2



# 生产者配置
# 消费者broker地址 多个地址用英文逗号分隔 优先级比spring.kafka.bootstrap-servers高，当不设置时，使用spring.kafka.bootstrap-servers的值
spring.kafka.producer.bootstrap-servers=host1:port1,host2:port2

# 生产者要求数据有多少个副本接收到数据才算发送成功
# 0：表示生产者数据发送到leader就算写入成功，但是如果leader在把数据写到本地磁盘时报错，就会数据丢失，akcs设置为0时，kafka可以达到最大的吞吐量
# 1：表示生产者数据发送到leader并写入到磁盘才算写入成功，但是如果数据在同步到其他副本时，leader挂了，其他副本被选举为新leader，那么就会有数据丢失
# -1 或者 all：表示生产者把数据发送到leader，并同步到其他副本，才算数据写入成功，这种模式一般不会产生数据丢失，但是kafka的吞吐量会很低
spring.kafka.producer.acks=1

# 生产者批量发送大小 单位：字节 默认16384字节（16KB）
spring.kafka.producer.batch-size=16384

# 发送缓冲区的大小，生产者可以用来缓冲等待发送到服务器的记录的内存总字节数 单位：字节 默认33554432字节（32MB）
spring.kafka.producer.buffer-memory=33554432

# 生产者发送频率，控制消息生产者在发送批次之前等待更多消息加入批次的时间，默认0ms，即没有延迟，立即发送  和batch-size参数满足任一条件发送
spring.kafka.producer.linger-ms=5

# 生产者发送消息体大小限制，生产者发送消息的最大大小 单位：字节 默认1048576字节（1MB）
spring.kafka.producer.properties.max.request.size=1048576

# 生产者请求超时时间，默认30000ms
spring.kafka.producer.request-timeout-ms=30000

# 生产者生成的所有数据的压缩类型 默认为none 压缩最好用于批量处理，批量处理消息越多，压缩性能越好
# gzip：
# snappy：
# lz4：
spring.kafka.producer.compression-type=gzip

# 当生产者数据发送失败时，可以重试发送的次数
spring.kafka.producer.retries=3

# 生产者key序列化方式 默认StringSerializer
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer

# 生产者消息序列化方式 默认StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer



# 消费者配置
# 消费者broker地址 多个地址用英文逗号分隔 优先级比spring.kafka.bootstrap-servers高，当不设置时，使用spring.kafka.bootstrap-servers的值
#spring.kafka.consumer.bootstrap-servers=host1:port1,host2:port2

# 消费者分组ID
spring.kafka.consumer.group-id=my-group

# 当Kafka中没有初始偏移量或者服务器上不再存在当前偏移量时,消费者读取消息的位置 默认值为latest
# earliest：从最早的 offset 开始读取数据，即从最早的消息开始消费，如果 offset 没有被记录，那么它会重置 offset 并从最早的消息开始消费
# latest：从最近的 offset 开始读取数据，即只有当前 consumerGroup 之后产生的数据才会被消费
# none：不会自动提交，必须手动调用 commit() 手动提交，但是应用程序不需要重新处理旧的数据，新的数据是继续消费的
spring.kafka.consumer.auto-offset-reset=earliest

# 消费者key反序列化方式 默认StringDeserializer
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# 消费者消息反序列化方式 默认StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

# 消费者的消费记录offset是否后台自动提交，默认值是true
spring.kafka.consumer.enable-auto-commit=false

# 当消费者的消费记录offset后台自动提交时，多长时间自动提交一次 单位：毫秒
spring.kafka.consumer.auto-commit-interval=1000

# 每个批次中最大的记录数，从一个分区中获取的消息数量，用于控制消费者拉取数据的频率和批量大小 默认500
spring.kafka.consumer.max-poll-records=500

# 消费者的最大空闲时间，默认300000ms
spring.kafka.consumer.max-poll-interval-ms=300000

# 消费者接收缓存区大小，默认2048KB
spring.kafka.consumer.receive-buffer-bytes=2048

# 消费者请求超时时间，默认30000ms
spring.kafka.consumer.request-timeout-ms=30000



# 消费者监听器配置
# 监听类型 默认SINGLE
# SINGLE：一次调用一个ConsumerRecord的端点
# BATCH：用一批ConsumerRecords调用端点
spring.kafka.listener.type=SINGLE

# 当消费者enable-auto-commit设置为false时，消费者的手动提交方式
# RECORD：当每一条记录被消费者监听器处理之后提交
# BATCH：当每一批poll()的数据被消费者监听器处理之后提交
# TIME：当每一批poll()的数据被消费者监听器处理之后，距离上次提交时间大于TIME时提交
# COUNT：当每一批poll()的数据被消费者监听器处理之后，被处理record数量大于等于COUNT时提交
# COUNT_TIME：TIME 或 COUNT 有一个条件满足时提交
# MANUAL：当每一批poll()的数据被消费者监听器处理之后, 手动调用Acknowledgment.acknowledge()后提交
# MANUAL_IMMEDIATE：手动调用Acknowledgment.acknowledge()后立即提交
spring.kafka.listener.ack-mode=RECORD

# 监听器容器中运行的线程数，表示启动多少个并发的消费者，这个值不能大于实际消费的主题的分区数
spring.kafka.listener.concurrency=3

# 消费者一次poll方法的超时时间，当在一次poll方法中，如果一次请求不到数据或者请求的数据小于设定的值，那么poll方法会继续执行请求，直到超时或者满足设置的条件 单位：毫秒，默认是5000
spring.kafka.listener.poll-timeout=5000

# 当 spring.kafka.listener.ack-mode 设置为 AckMode.COUNT 或者 AckMode.COUNT_TIME 时生效，表示当一个poll数据消费后，处理的记录数大于多少时，触发提交
spring.kafka.listener.ack-count=100

# 当 spring.kafka.listener.ack-mode 设置为 AckMode.TIME  或者 AckMode.COUNT_TIME 时生效，表示当一个poll数据消费后，距离上次提交时间大于 ack-time 时提交
spring.kafka.listener.ack-time=1000

# 业务配置
# 业务topic配置
spring.kafka.biz.topic=test-topic

